<html><!-- Created using the cpp_pretty_printer from the dlib C++ library.  See http://dlib.net for updates. --><head><title>dlib C++ Library - structural_svm_problem_abstract.h</title></head><body bgcolor='white'><pre>
<font color='#009900'>// Copyright (C) 2011  Davis E. King (davis@dlib.net)
</font><font color='#009900'>// License: Boost Software License   See LICENSE.txt for the full license.
</font><font color='#0000FF'>#undef</font> DLIB_STRUCTURAL_SVM_PRObLEM_ABSTRACT_Hh_
<font color='#0000FF'>#ifdef</font> DLIB_STRUCTURAL_SVM_PRObLEM_ABSTRACT_Hh_

<font color='#0000FF'>#include</font> "<a style='text-decoration:none' href='../optimization/optimization_oca_abstract.h.html'>../optimization/optimization_oca_abstract.h</a>"
<font color='#0000FF'>#include</font> "<a style='text-decoration:none' href='sparse_vector_abstract.h.html'>sparse_vector_abstract.h</a>"
<font color='#0000FF'>#include</font> "<a style='text-decoration:none' href='../matrix.h.html'>../matrix.h</a>"

<font color='#0000FF'>namespace</font> dlib
<b>{</b>

<font color='#009900'>// ----------------------------------------------------------------------------------------
</font>
    <font color='#0000FF'>template</font> <font color='#5555FF'>&lt;</font>
        <font color='#0000FF'>typename</font> matrix_type_,
        <font color='#0000FF'>typename</font> feature_vector_type_ <font color='#5555FF'>=</font> matrix_type_
        <font color='#5555FF'>&gt;</font>
    <font color='#0000FF'>class</font> <b><a name='structural_svm_problem'></a>structural_svm_problem</b> : <font color='#0000FF'>public</font> oca_problem<font color='#5555FF'>&lt;</font>matrix_type_<font color='#5555FF'>&gt;</font> 
    <b>{</b>
    <font color='#0000FF'>public</font>:
        <font color='#009900'>/*!
            REQUIREMENTS ON matrix_type_
                - matrix_type_ == a dlib::matrix capable of storing column vectors

            REQUIREMENTS ON feature_vector_type_ 
                - feature_vector_type_ == a dlib::matrix capable of storing column vectors
                  or an unsorted sparse vector type as defined in dlib/svm/sparse_vector_abstract.h.

            INITIAL VALUE
                - get_epsilon() == 0.001
                - get_max_iterations() == 10000
                - get_max_cache_size() == 5
                - get_c() == 1
                - get_cache_based_epsilon() == std::numeric_limits&lt;scalar_type&gt;::infinity()
                  (I.e. the cache based epsilon feature is disabled)
                - num_nuclear_norm_regularizers() == 0
                - This object will not be verbose

            WHAT THIS OBJECT REPRESENTS
                This object is a tool for solving the optimization problem associated with
                a structural support vector machine.  A structural SVM is a supervised
                machine learning method for learning to predict complex outputs.  This is
                contrasted with a binary classifier which makes only simple yes/no
                predictions.  A structural SVM, on the other hand, can learn to predict
                complex outputs such as entire parse trees or DNA sequence alignments.  To
                do this, it learns a function F(x,y) which measures how well a particular
                data sample x matches a label y.  When used for prediction, the best label
                for a new x is given by the y which maximizes F(x,y).   

                To use this object you inherit from it, provide implementations of its four
                pure virtual functions, and then pass your object to the oca optimizer.
                Also, you should only pass an instance of this object to the oca optimizer
                once.  That is, the act of using a structural_svm_problem instance with the
                oca solver "uses" the structural_svm_problem instance.  If you want to
                solve the same problem multiple times then you must use a fresh instance of
                your structural_svm_problem.


                To define the optimization problem precisely, we first introduce some notation:
                    - let PSI(x,y)    == the joint feature vector for input x and a label y.
                    - let F(x,y|w)    == dot(w,PSI(x,y)).  
                    - let LOSS(idx,y) == the loss incurred for predicting that the idx-th training 
                      sample has a label of y.  Note that LOSS() should always be &gt;= 0 and should
                      become exactly 0 when y is the correct label for the idx-th sample.
                    - let x_i == the i-th training sample.
                    - let y_i == the correct label for the i-th training sample.
                    - The number of data samples is N.

                Then the optimization problem solved using this object is the following:
                    Minimize: h(w) == 0.5*dot(w,w) + C*R(w)

                    Where R(w) == sum from i=1 to N: 1/N * sample_risk(i,w)
                    and sample_risk(i,w) == max over all Y: LOSS(i,Y) + F(x_i,Y|w) - F(x_i,y_i|w)
                    and C &gt; 0

                

                For an introduction to structured support vector machines you should consult 
                the following paper: 
                    Predicting Structured Objects with Support Vector Machines by 
                    Thorsten Joachims, Thomas Hofmann, Yisong Yue, and Chun-nam Yu

                For a more detailed discussion of the particular algorithm implemented by this
                object see the following paper:  
                    T. Joachims, T. Finley, Chun-Nam Yu, Cutting-Plane Training of Structural SVMs, 
                    Machine Learning, 77(1):27-59, 2009.

                    Note that this object is essentially a tool for solving the 1-Slack structural
                    SVM with margin-rescaling.  Specifically, see Algorithm 3 in the above referenced 
                    paper.
        !*/</font>

        <font color='#0000FF'>typedef</font> matrix_type_ matrix_type;
        <font color='#0000FF'>typedef</font> <font color='#0000FF'>typename</font> matrix_type::type scalar_type;
        <font color='#0000FF'>typedef</font> feature_vector_type_ feature_vector_type;

        <b><a name='structural_svm_problem'></a>structural_svm_problem</b> <font face='Lucida Console'>(</font>
        <font face='Lucida Console'>)</font>;
        <font color='#009900'>/*!
            ensures
                - this object is properly initialized
        !*/</font>

        <font color='#0000FF'><u>void</u></font> <b><a name='set_epsilon'></a>set_epsilon</b> <font face='Lucida Console'>(</font>
            scalar_type eps
        <font face='Lucida Console'>)</font>;
        <font color='#009900'>/*!
            requires
                - eps &gt; 0
            ensures
                - #get_epsilon() == eps
        !*/</font>

        <font color='#0000FF'>const</font> scalar_type <b><a name='get_epsilon'></a>get_epsilon</b> <font face='Lucida Console'>(</font>
        <font face='Lucida Console'>)</font> <font color='#0000FF'>const</font>;
        <font color='#009900'>/*!
            ensures
                - returns the error epsilon that determines when training should stop.
                  Smaller values may result in a more accurate solution but take longer 
                  to execute.  Specifically, the algorithm stops when the average sample
                  risk (i.e. R(w) as defined above) is within epsilon of its optimal value.

                  Also note that sample risk is an upper bound on a sample's loss.  So
                  you can think of this epsilon value as saying "solve the optimization
                  problem until the average loss per sample is within epsilon of it's 
                  optimal value".
        !*/</font>

        scalar_type <b><a name='get_cache_based_epsilon'></a>get_cache_based_epsilon</b> <font face='Lucida Console'>(</font>
        <font face='Lucida Console'>)</font> <font color='#0000FF'>const</font>;
        <font color='#009900'>/*!
            ensures
                - if (get_max_cache_size() != 0) then
                    - The solver will not stop when the average sample risk is within
                      get_epsilon() of its optimal value.  Instead, it will keep running
                      but will run the optimizer completely on the cache until the average
                      sample risk is within #get_cache_based_epsilon() of its optimal
                      value.  This means that it will perform this additional refinement in
                      the solution accuracy without making any additional calls to the
                      separation_oracle().  This is useful when using a nuclear norm
                      regularization term because it allows you to quickly solve the
                      optimization problem to a high precision, which in the case of a
                      nuclear norm regularized problem means that many of the learned
                      matrices will be low rank or very close to low rank due to the
                      nuclear norm regularizer.  This may not happen without solving the
                      problem to a high accuracy or their ranks may be difficult to
                      determine, so the extra accuracy given by the cache based refinement
                      is very useful.  Finally, note that we include the nuclear norm term
                      as part of the "risk" for the purposes of determining when to stop.  
                - else
                    - The value of #get_cache_based_epsilon() has no effect.
        !*/</font>

        <font color='#0000FF'><u>void</u></font> <b><a name='set_cache_based_epsilon'></a>set_cache_based_epsilon</b> <font face='Lucida Console'>(</font>
            scalar_type eps
        <font face='Lucida Console'>)</font>;
        <font color='#009900'>/*!
            requires
                - eps &gt; 0
            ensures
                - #get_cache_based_epsilon() == eps
        !*/</font>

        <font color='#0000FF'><u>void</u></font> <b><a name='set_max_iterations'></a>set_max_iterations</b> <font face='Lucida Console'>(</font>
            <font color='#0000FF'><u>unsigned</u></font> <font color='#0000FF'><u>long</u></font> max_iter
        <font face='Lucida Console'>)</font>;
        <font color='#009900'>/*!
            ensures
                - #get_max_iterations() == max_iter
        !*/</font>

        <font color='#0000FF'><u>unsigned</u></font> <font color='#0000FF'><u>long</u></font> <b><a name='get_max_iterations'></a>get_max_iterations</b> <font face='Lucida Console'>(</font>
        <font face='Lucida Console'>)</font>; 
        <font color='#009900'>/*!
            ensures
                - returns the maximum number of iterations the SVM optimizer is allowed to
                  run before it is required to stop and return a result.
        !*/</font>

        <font color='#0000FF'><u>void</u></font> <b><a name='set_max_cache_size'></a>set_max_cache_size</b> <font face='Lucida Console'>(</font>
            <font color='#0000FF'><u>unsigned</u></font> <font color='#0000FF'><u>long</u></font> max_size
        <font face='Lucida Console'>)</font>;
        <font color='#009900'>/*!
            ensures
                - #get_max_cache_size() == max_size
        !*/</font>

        <font color='#0000FF'><u>unsigned</u></font> <font color='#0000FF'><u>long</u></font> <b><a name='get_max_cache_size'></a>get_max_cache_size</b> <font face='Lucida Console'>(</font>
        <font face='Lucida Console'>)</font> <font color='#0000FF'>const</font>; 
        <font color='#009900'>/*!
            ensures
                - Returns the number of joint feature vectors per training sample kept in 
                  the separation oracle cache.  This cache is used to avoid unnecessary 
                  calls to the user supplied separation_oracle() function.  Note that a 
                  value of 0 means that caching is not used at all.  This is appropriate 
                  if the separation oracle is cheap to evaluate. 
        !*/</font>

        <font color='#0000FF'><u>void</u></font> <b><a name='add_nuclear_norm_regularizer'></a>add_nuclear_norm_regularizer</b> <font face='Lucida Console'>(</font>
            <font color='#0000FF'><u>long</u></font> first_dimension,
            <font color='#0000FF'><u>long</u></font> rows,
            <font color='#0000FF'><u>long</u></font> cols,
            <font color='#0000FF'><u>double</u></font> regularization_strength
        <font face='Lucida Console'>)</font>;
        <font color='#009900'>/*!
            requires
                - 0 &lt;= first_dimension &lt; get_num_dimensions()
                - 0 &lt;= rows
                - 0 &lt;= cols
                - first_dimension+rows*cols &lt;= get_num_dimensions()
                - 0 &lt; regularization_strength
            ensures
                - Adds a nuclear norm regularization term to the optimization problem
                  solved by this object.  That is, instead of solving:
                    Minimize: h(w) == 0.5*dot(w,w) + C*R(w)
                  this object will solve:
                    Minimize: h(w) == 0.5*dot(w,w) + C*R(w) + regularization_strength*nuclear_norm_of(part of w)
                  where "part of w" is the part of w indicated by the arguments to this
                  function. In particular, the part of w included in the nuclear norm is
                  exactly the matrix reshape(rowm(w, range(first_dimension, first_dimension+rows*cols-1)), rows, cols).
                  Therefore, if you think of the w vector as being the concatenation of a
                  bunch of matrices then you can use multiple calls to add_nuclear_norm_regularizer() 
                  to add nuclear norm regularization terms to any of the matrices packed into w.
                - #num_nuclear_norm_regularizers() == num_nuclear_norm_regularizers() + 1
        !*/</font>

        <font color='#0000FF'><u>unsigned</u></font> <font color='#0000FF'><u>long</u></font> <b><a name='num_nuclear_norm_regularizers'></a>num_nuclear_norm_regularizers</b> <font face='Lucida Console'>(</font>
        <font face='Lucida Console'>)</font> <font color='#0000FF'>const</font>; 
        <font color='#009900'>/*!
            ensures
                - returns the number of nuclear norm regularizers that are currently a part
                  of this optimization problem.  That is, returns the number of times
                  add_nuclear_norm_regularizer() has been called since the last call to
                  clear_nuclear_norm_regularizers() or object construction, whichever is
                  most recent.
        !*/</font>

        <font color='#0000FF'><u>void</u></font> <b><a name='clear_nuclear_norm_regularizers'></a>clear_nuclear_norm_regularizers</b> <font face='Lucida Console'>(</font>
        <font face='Lucida Console'>)</font>;
        <font color='#009900'>/*!
            ensures
                - #num_nuclear_norm_regularizers() == 0
        !*/</font>

        <font color='#0000FF'><u>void</u></font> <b><a name='be_verbose'></a>be_verbose</b> <font face='Lucida Console'>(</font>
        <font face='Lucida Console'>)</font>;
        <font color='#009900'>/*!
            ensures
                - This object will print status messages to standard out so that a 
                  user can observe the progress of the algorithm.
        !*/</font>

        <font color='#0000FF'><u>void</u></font> <b><a name='be_quiet'></a>be_quiet</b><font face='Lucida Console'>(</font>
        <font face='Lucida Console'>)</font>;
        <font color='#009900'>/*!
            ensures
                - this object will not print anything to standard out
        !*/</font>

        scalar_type <b><a name='get_c'></a>get_c</b> <font face='Lucida Console'>(</font>
        <font face='Lucida Console'>)</font> <font color='#0000FF'>const</font>; 
        <font color='#009900'>/*!
            ensures
                - returns the SVM regularization parameter.  It is the parameter that 
                  determines the trade off between trying to fit the training data 
                  exactly or allowing more errors but hopefully improving the 
                  generalization of the resulting classifier.  Larger values encourage 
                  exact fitting while smaller values of C may encourage better 
                  generalization. 
        !*/</font>

        <font color='#0000FF'><u>void</u></font> <b><a name='set_c'></a>set_c</b> <font face='Lucida Console'>(</font>
            scalar_type C
        <font face='Lucida Console'>)</font>;
        <font color='#009900'>/*!
            requires
                - C &gt; 0
            ensures
                - #get_c() == C
        !*/</font>

    <font color='#009900'>// --------------------------------
</font>    <font color='#009900'>//     User supplied routines
</font>    <font color='#009900'>// --------------------------------
</font>
        <font color='#0000FF'>virtual</font> <font color='#0000FF'><u>long</u></font> <b><a name='get_num_dimensions'></a>get_num_dimensions</b> <font face='Lucida Console'>(</font>
        <font face='Lucida Console'>)</font> <font color='#0000FF'>const</font> <font color='#5555FF'>=</font> <font color='#979000'>0</font>;
        <font color='#009900'>/*!
            ensures
                - returns the dimensionality of a joint feature vector
        !*/</font>

        <font color='#0000FF'>virtual</font> <font color='#0000FF'><u>long</u></font> <b><a name='get_num_samples'></a>get_num_samples</b> <font face='Lucida Console'>(</font>
        <font face='Lucida Console'>)</font> <font color='#0000FF'>const</font> <font color='#5555FF'>=</font> <font color='#979000'>0</font>;
        <font color='#009900'>/*!
            ensures
                - returns the number of training samples in this problem. 
        !*/</font>

        <font color='#0000FF'>virtual</font> <font color='#0000FF'><u>void</u></font> <b><a name='get_truth_joint_feature_vector'></a>get_truth_joint_feature_vector</b> <font face='Lucida Console'>(</font>
            <font color='#0000FF'><u>long</u></font> idx,
            feature_vector_type<font color='#5555FF'>&amp;</font> psi 
        <font face='Lucida Console'>)</font> <font color='#0000FF'>const</font> <font color='#5555FF'>=</font> <font color='#979000'>0</font>;
        <font color='#009900'>/*!
            requires
                - 0 &lt;= idx &lt; get_num_samples()
            ensures
                - #psi == PSI(x_idx, y_idx)
                  (i.e. the joint feature vector for the idx-th training sample its true label.)
        !*/</font>

        <font color='#0000FF'>virtual</font> <font color='#0000FF'><u>void</u></font> <b><a name='separation_oracle'></a>separation_oracle</b> <font face='Lucida Console'>(</font>
            <font color='#0000FF'>const</font> <font color='#0000FF'><u>long</u></font> idx,
            <font color='#0000FF'>const</font> matrix_type<font color='#5555FF'>&amp;</font> current_solution,
            scalar_type<font color='#5555FF'>&amp;</font> loss,
            feature_vector_type<font color='#5555FF'>&amp;</font> psi
        <font face='Lucida Console'>)</font> <font color='#0000FF'>const</font> <font color='#5555FF'>=</font> <font color='#979000'>0</font>;
        <font color='#009900'>/*!
            requires
                - 0 &lt;= idx &lt; get_num_samples()
                - current_solution.size() == get_num_dimensions()
            ensures
                - runs the separation oracle on the idx-th sample.  We define this as follows: 
                    - let X           == the idx-th training sample.
                    - let PSI(X,y)    == the joint feature vector for input X and an arbitrary label y.
                    - let F(X,y)      == dot(current_solution,PSI(X,y)).  
                    - let LOSS(idx,y) == the loss incurred for predicting that the idx-th sample
                      has a label of y.  Note that LOSS() should always be &gt;= 0 and should
                      become exactly 0 when y is the correct label for the idx-th sample.

                        Then the separation oracle finds a Y such that: 
                            Y = argmax over all y: LOSS(idx,y) + F(X,y) 
                            (i.e. It finds the label which maximizes the above expression.)

                        Finally, we can define the outputs of this function as:
                        - #loss == LOSS(idx,Y) 
                        - #psi == PSI(X,Y) 
        !*/</font>
    <b>}</b>;

<font color='#009900'>// ----------------------------------------------------------------------------------------
</font>
<b>}</b>

<font color='#0000FF'>#endif</font> <font color='#009900'>// DLIB_STRUCTURAL_SVM_PRObLEM_ABSTRACT_Hh_
</font>


</pre></body></html>